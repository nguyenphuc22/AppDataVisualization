from DataManager.DataManager import AbstractDataManager

import pandas as pd
import re
from underthesea import word_tokenize
from datetime import datetime
import logging

class ReviewManager(AbstractDataManager):
    _instance = None
    
    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super(ReviewManager, cls).__new__(cls, *args, **kwargs)
        return cls._instance
    
    def __init__(self):
        super().__init__()
        self.vietnamese_stopwords = self.load_stopwords('./Data/vietnamese-stopwords.txt')
        self.compound_words = self.load_compound_words('./Data/compound_words.txt')
        self.positive_words = [
            'nhanh ch√≥ng', 'hi·ªáu qu·∫£', 'b·ªÅn b·ªâ', 'ƒë√°p ·ª©ng', 't·ªët', 'h·ª£p l√Ω', 'h√†i l√≤ng', 'tin t∆∞·ªüng', 'an t√¢m', 'c·∫£m ∆°n',
            'tuy·ªát v·ªùi', 'uy t√≠n', 't√≠ch c·ª±c', '5 sao', 'h√†ng ƒë·∫πp', 'ƒë·∫πp', 'm·ªõi', 'nhanh', 'm∆∞·ª£t', 'pin t·ªët', 'm·∫°nh', 'pin kh·ªèe',
            'nhi·ªát t√¨nh', 'ch·∫•t l∆∞·ª£ng', '∆∞ng √Ω', 'chu ƒë√°o', '·ªßng h·ªô', 'chu·∫©n', 'b·ªÅn', 'ph√π h·ª£p', 'th√∫ v·ªã', 'r·∫ª', 'mong ƒë·ª£i',
            'y√™n t√¢m', '∆∞ng √Ω', 'hay', 'xu·∫•t s·∫Øc', 'ƒë√∫ng', 'b·ªÅn', 'chu·∫©n', '·ªïn ƒë·ªãnh', 'c·∫©n th·∫≠n', 'ƒë·∫ßy ƒë·ªß', 'x·ª©ng ƒë√°ng', 'may m·∫Øn',
            'b·ªï r·∫ª', 'chuy√™n nghi·ªáp', '∆∞ng √Ω', 'sang', 'ƒë√∫ng', '·ªïn √°p', '√™m ru', 'tot'
        ]
        self.negative_words = [
            'k√©m', 'gian', 'l·∫≠n', 'gian l·∫≠n','l√πa ƒë·∫£o', 'l·ª´a ƒë·∫£o', 'l·ª´a g·∫°t', 'm·∫•t ƒë·∫°o ƒë·ª©c', 'x·∫°o',
            'kh√¥ng ƒë√∫ng', 'kh√¥ng c√≥', 'hu hong', 'mau h·∫øt', 'ch√°n', '1 sao', 'l·ªói', 'kh√¥ng mua', 'h√†ng d·ªüm', 'm√°y l·ªói',
            'n√≥ng m√°u', 'm√π', 'l·ª´a', 'y·∫øu', 'gi·ªüm', 'h√†ng c≈©', 'ko ƒë√∫ng', 'k√©m', 'li·ªát pin', 'nh·∫•p nh√°y', 'h∆∞', 'treo ƒë·∫ßu d√™ b√°n th·ªãt ch√≥',
            'ch√™', 'k ƒë√∫ng', 'l·ªïi', 'c≈©', 'th·∫•t v·ªçng', 'ph√≠ ti·ªÅn', 'c≈© x∆∞·ªõc', 'h√†ng t·ªá', 'ko trung th·ª±c', 'r√πa b√≤', 'ch·∫≠m', 'kh√¥ng nhi·ªát t√¨nh',
            'kh√¥ng ho·∫°t ƒë·ªông', 'tr√°o h√†ng', 'h·∫øt pin', 'h·∫øt dung l∆∞·ª£ng', 'd·ªèm', 'r√°c', 't√™ li·ªát', 'l√£ng ph√≠', 'ph√≠ ti·ªÅn', 'c·ªë √Ω',
            'kh√¥ng tr·∫£ l·ªùi', 'xau', 'giao sai', 'pin mau h·∫øt', 'lua dao', 'not working', 'poor quality', 'ƒë∆°', 'ƒë·ªÉu', 'l·ª´a l·ªçc',
            'gi·∫£ m·∫°o', 'te', 'cham', 'it', 'b·ª±c m√¨nh', 't·ª´ ch·ªëi', 'ch·∫≠m tr·ªÖ', 's∆° s√†i', 'l∆∞a', 'tr·∫ßy', 'pin y·∫øu', 'kh√¥ng ph·∫£n h·ªìi',
            'd·ªïm', 'l∆∞a ƒëao', 'ko dung', 's·∫≠p', 'm·∫•t ti·ªÅn', 'b·ª±c', 'm·∫•t', 'c·∫£nh gi√°c', 'l√£ng ph√≠', 'l√¥i', 'giao sai', 'v·ª°', 'yeu',
            'cham', 'kem', 'ko h√†i l√≤ng', 'sai', '0', 'b·∫©n', 'thua', 'kh√¥ng t·ªët', 'h·ªèng', 'l√°c'
        ]
        self.neutral_words = [
            'b√¨nh th∆∞·ªùng', 'trung', 'b√¨nh', '·ªïn', 'b√¨nh', 'd√¢n', 'bt', 'th√¥ng th∆∞·ªùng', 'nan'
        ]
        self.brand_map = {
            'Apple': ['iphone', 'lphone', 'ip', 'kh·ªßng lphone', 'i', 'i15', 'i14', 'air', 'lpad', 'ipad', 'pro', 'iph∆°ne', 'ip6', 'ip6g', 'padmini', 'pad4', 'pad', 'ipaad', 'iph0ne', 'pad4 b·∫£n'],
            'Samsung': ['galaxy', 'samsung', 'a57', 'ss', 's23', 'ultra' ,'tho·∫°ia71', 's22ultra', 'ƒë·ªìng s22ultra', 'tab3', 'tab'],
            'Google': ['pixel'],
            'Huawei': ['huawei', 'honor'],
            'Lenovo': ['lenovo'],
            'LG': ['lg'],
            'Nokia': ['nokia', 'e71'],
            'Nobard': ['nobard'],
            'MASSTEL': ['masstel'],
            'Oppo': ['oppo', 'a56', 'tho·∫°ia58', 'tho·∫°ia57', 'tho·∫°ia77', 'a95', 'reno', 'tho·∫°ia93', 'tho·∫°ia17', 'tho·∫°ia38', 'a58', 'tho·∫°if11', 'a73', 'a93' ,'tho·∫°ia73', 'a18', 'a3s', 'tho·∫°ia74', 'a2x', 'a17'],
            'Realme': ['realme'],
            'Sony': ['sony'],
            'Vivo': ['vivo', 'tho·∫°iy50', 'tho·∫°iy17', 'tho·∫°iy19', 'tho·∫°iv20', 'tho·∫°iy50', 'tho·∫°iy55', 'tho·∫°iy22s', 'v29', 'tho·∫°iy11', 'tho·∫°iy93', 'y56', 'y19', 'tho·∫°iy16', 'tho·∫°iy11', 'y93', 'y70s', 'y22s'],
            'Vsmart': ['vsmart'],
            'Xiaomi': ['xiaomi', 'redmi'],
            'Sharp': ['sharp'],
            'Gree': ['gree'],
            'Murah': ['murah']
        }
        # Thi·∫øt l·∫≠p logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

    def load_stopwords(self, file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            return set(f.read().splitlines())

    # Load d·ªØ li·ªáu compound words
    def load_compound_words(self, file_path):
        with open(file_path, 'r', encoding='utf-8') as file:
            return set(line.strip() for line in file)

    # L·ªçc c√°c bi·ªÉu c·∫£m trong vƒÉn b·∫£n
    def remove_emojis(self, text):
        emoji_pattern = re.compile("["
                                   u"\U0001F600-\U0001F64F"  # emoticons
                                   u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                                   u"\U0001F680-\U0001F6FF"  # transport & map symbols
                                   u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                                   u"\U00002702-\U000027B0"
                                   u"\U000024C2-\U0001F251"
                                   "]+", flags=re.UNICODE)
        return emoji_pattern.sub(r'', text)

    # Lo·∫°i b·ªè d·∫•u c√¢u
    def remove_punctuation(self, text):
        return re.sub(r'[^\w\s]', '', text)

    # Thay th·∫ø icon sao b·∫±ng t·ª´ "sao"
    def replace_star_icons(self, text):
        star_pattern = re.compile(r'[*‚òÖ‚òÜ‚≠êüåü‚ú®]')
        return star_pattern.sub(' sao', text)
    
    # Thay th·∫ø d·∫•u ch·∫•m nhi·ªÅu l·∫ßn b·∫±ng m·ªôt d·∫•u c√°ch
    def replace_dots(self, text):
        return re.sub(r'\.{2,}', ' ', text)    

    # K·∫øt h·ª£p c√°c t·ª´ gh√©p
    def combine_compound_words(self, words):
        combined_words = []
        i = 0
        while i < len(words):
            if i < len(words) - 1:
                two_word_combo = words[i] + ' ' + words[i + 1]
                if two_word_combo in self.compound_words:
                    combined_words.append(two_word_combo.replace(' ', '_'))
                    i += 2
                    continue
            combined_words.append(words[i])
            i += 1
        return combined_words

    # Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu
    def preprocessing(self, text):
        text = str(text).lower()
        text = self.replace_dots(text)
        text = self.replace_star_icons(text)
        text = self.remove_emojis(text)
        text = self.remove_punctuation(text)
        words = word_tokenize(text, format='text').split()
        words = [w for w in words if w not in self.vietnamese_stopwords]
        words = self.combine_compound_words(words)
        words = [w.replace('_', ' ') for w in words]
        return words

    # Ph√¢n lo·∫°i c·∫£m x√∫c
    def classify_sentiment(self, keywords):
        positive_count = sum(1 for word in keywords if word in self.positive_words)
        negative_count = sum(1 for word in keywords if word in self.negative_words)
        neutral_count = sum(1 for word in keywords if word in self.neutral_words)

        if positive_count > negative_count and positive_count > neutral_count:
            return 'Positive'
        elif negative_count > positive_count and negative_count > neutral_count:
            return 'Negative'
        else:
            return 'Neutral'

    # Tr√≠ch xu·∫•t th∆∞∆°ng hi·ªáu
    def extract_brand(self, cleaned_item):
        for brand, keywords in self.brand_map.items():
            for keyword in keywords:
                if keyword in cleaned_item:
                    return brand
        return 'None'

    # Load d·ªØ li·ªáu
    def preprocess_data(self, data):
        print("Preprocessing data entry ReviewManager")

        # ƒê·ªãnh d·∫°ng l·∫°i ng√†y 
        data['boughtDate'] = pd.to_datetime(data['boughtDate'], format='%Y%m%d', errors='coerce')

        # Remove duplicates c√°c ƒë√°nh gi√° 5 sao (seeding)
        df_rating_5 = data[data['rating'] == 5]
        df_rating_5_unique = df_rating_5.drop_duplicates(subset=['reviewContent'])
        data = pd.concat([data[data['rating'] != 5], df_rating_5_unique])
        data.reset_index(drop=True, inplace=True)

        # √Åp d·ª•ng h√†m preprocessing
        data['cleanedContent'] = data['reviewContent'].apply(self.preprocessing)
        data['cleanedItem'] = data['itemTitle'].apply(self.preprocessing)

        # Ph√¢n lo·∫°i c·∫£m x√∫c
        data['Sentiment'] = data['cleanedContent'].apply(self.classify_sentiment)

        # Tr√≠ch xu·∫•t th∆∞∆°ng hi·ªáu
        data['brandName'] = data['cleanedItem'].apply(self.extract_brand)

        # Drop c√°c c·ªôt kh√¥ng c·∫ßn thi·∫øt
        data.drop(columns=['cleanedItem'], inplace=True)
        return data

    def get_data(self):
        """
        Tr·∫£ v·ªÅ d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω.
        """
        if self.data is None:
            self.logger.warning("Data has not been loaded. Call load_data() first.")
        return self.data

    def get_data_summary(self):
        """
        Tr·∫£ v·ªÅ t√≥m t·∫Øt v·ªÅ d·ªØ li·ªáu.
        """
        if self.data is None:
            return "Data has not been loaded."
        return f"Data shape: {self.data.shape}, Columns: {', '.join(self.data.columns)}"
    